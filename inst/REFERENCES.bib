@article{gglasso,
	author = {Yang, Yi and Zou, Hui},
	title = "{A fast unified algorithm for solving group-lasso penalize learning problems}",
	volume = {25},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-014-9498-5},
	doi = {10.1007/s11222-014-9498-5},
	abstract = {This paper concerns a class of group-lasso learning problems where the objective function is the sum of an empirical loss and the group-lasso penalty. For a class of loss function satisfying a quadratic majorization condition, we derive a unified algorithm called groupwise-majorization-descent (GMD) for efficiently computing the solution paths of the corresponding group-lasso penalized learning problem. GMD allows for general design matrices, without requiring the predictors to be group-wise orthonormal. As illustration examples, we develop concrete algorithms for solving the group-lasso penalized least squares and several group-lasso penalized large margin classifiers. These group-lasso models have been implemented in an R package gglasso publicly available from the Comprehensive R Archive Network (CRAN) at http://cran.r-project.org/web/packages/gglasso. On simulated and real data, gglasso consistently outperforms the existing software for computing the group-lasso that implements either the classical groupwise descent algorithm or Nesterov’s method.},
	language = {en},
	number = {6},
	urldate = {2022-09-06},
	journal = {Statistics and Computing},
	month = nov,
	year = {2015},
	keywords = {Group lasso, Groupwise descent, grplasso, Large margin classifiers, MM principle, SLEP},
	pages = {1129--1141},
}
@article{lasso-laplacian,
    author = {Li, Caiyan and Li, Hongzhe},
    title = "{Network-constrained regularization and variable selection for analysis of genomic data}",
    journal = {Bioinformatics},
    volume = {24},
    number = {9},
    pages = {1175-1182},
    year = {2008},
    month = {03},
    abstract = "{Motivation: Graphs or networks are common ways of depicting information. In biology in particular, many different biological processes are represented by graphs, such as regulatory networks or metabolic pathways. This kind of a priori information gathered over many years of biomedical research is a useful supplement to the standard numerical genomic data such as microarray gene-expression data. How to incorporate information encoded by the known biological networks or graphs into analysis of numerical data raises interesting statistical challenges. In this article, we introduce a network-constrained regularization procedure for linear regression analysis in order to incorporate the information from these graphs into an analysis of the numerical data, where the network is represented as a graph and its corresponding Laplacian matrix. We define a network-constrained penalty function that penalizes the L1-norm of the coefficients but encourages smoothness of the coefficients on the network.Results: Simulation studies indicated that the method is quite effective in identifying genes and subnetworks that are related to disease and has higher sensitivity than the commonly used procedures that do not use the pathway structure information. Application to one glioblastoma microarray gene-expression dataset identified several subnetworks on several of the Kyoto Encyclopedia of Genes and Genomes (KEGG) transcriptional pathways that are related to survival from glioblastoma, many of which were supported by published literatures.Conclusions: The proposed network-constrained regularization procedure efficiently utilizes the known pathway structures in identifying the relevant genes and the subnetworks that might be related to phenotype in a general regression framework. As more biological networks are identified and documented in databases, the proposed method should find more applications in identifying the subnetworks that are related to diseases and other biological processes.Contact:  hongzhe@mail.med.upenn.edu}",
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btn081},
    url = {https://doi.org/10.1093/bioinformatics/btn081},
}
@article{spatial-connectivity,
AUTHOR={Steiner, Aleksandra and Abbas, Kausar and Brzyski, Damian and Pączek, Kewin and Randolph, Timothy W. and Goñi, Joaquín and Harezlak, Jaroslaw},
TITLE={Incorporation of spatial- and connectivity-based cortical brain region information in regularized regression: Application to Human Connectome Project data},
JOURNAL={Frontiers in Neuroscience},
VOLUME={16},
YEAR={2022},
URL={https://www.frontiersin.org/articles/10.3389/fnins.2022.957282},
DOI={10.3389/fnins.2022.957282},
ISSN={1662-453X},
ABSTRACT={Studying the association of the brain's structure and function with neurocognitive outcomes requires a comprehensive analysis that combines different sources of information from a number of brain-imaging modalities. Recently developed regularization methods provide a novel approach using information about brain structure to improve the estimation of coefficients in the linear regression models. Our proposed method, which is a special case of the Tikhonov regularization, incorporates structural connectivity derived with Diffusion Weighted Imaging and cortical distance information in the penalty term. Corresponding to previously developed methods that inform the estimation of the regression coefficients, we incorporate additional information via a Laplacian matrix based on the proximity measure on the cortical surface. Our contribution consists of constructing a principled formulation of the penalty term and testing the performance of the proposed approach via extensive simulation studies and a brain-imaging application. The penalty term is constructed as a weighted combination of structural connectivity and proximity between cortical areas. Simulation studies mimic the real brain-imaging settings. We apply our approach to the study of data collected in the Human Connectome Project, where the cortical properties of the left hemisphere are found to be associated with vocabulary comprehension.}
}
@article{group-lasso,
    author = {Yuan, Ming and Lin, Yi},
    title = "{Model Selection and Estimation in Regression with Grouped Variables}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {68},
    number = {1},
    pages = {49-67},
    year = {2005},
    month = {12},
    abstract = "{We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.}",
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2005.00532.x},
    url = {https://doi.org/10.1111/j.1467-9868.2005.00532.x},
}
@Article{glmnet,
    title = {Regularization Paths for Generalized Linear Models via
      Coordinate Descent},
    author = {Jerome Friedman and Robert Tibshirani and Trevor Hastie},
    journal = {Journal of Statistical Software},
    year = {2010},
    volume = {33},
    number = {1},
    pages = {1--22},
    doi = {10.18637/jss.v033.i01},
  }
@article{lasso,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2023-10-25},
 volume = {58},
 year = {1996}
}
@Article{CH1,
  title = {Complex heatmaps reveal patterns and correlations in multidimensional genomic data},
  author = {Zuguang Gu and Roland Eils and Matthias Schlesner},
  journal = {Bioinformatics},
  doi = {10.1093/bioinformatics/btw313},
  year = {2016},
}
@Article{CH2,
  title = {Complex Heatmap Visualization},
  author = {Zuguang Gu},
  doi = {10.1002/imt2.43},
  journal = {iMeta},
  year = {2022},
}
@Article{CH3,
    title = {Make Interactive Complex Heatmaps in R},
    author = {Zuguang Gu and Daniel Huebschmann},
    journal = {Bioinformatics},
    year = {2021},
    doi = {10.1093/bioinformatics/btab806},
  }
@Book{ggplot2,
    author = {Hadley Wickham},
    title = {ggplot2: Elegant Graphics for Data Analysis},
    publisher = {Springer-Verlag New York},
    year = {2016},
    isbn = {978-3-319-24277-4},
    url = {https://ggplot2.tidyverse.org},
  }
@Book{plotly,
    author = {Carson Sievert},
    title = {Interactive Web-Based Data Visualization with R, plotly, and shiny},
    publisher = {Chapman and Hall/CRC},
    year = {2020},
    isbn = {9781138331457},
    url = {https://plotly-r.com},
  }
